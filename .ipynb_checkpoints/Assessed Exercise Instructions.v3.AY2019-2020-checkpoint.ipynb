{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CVMA (H) Assessed Exercise:  Image Transformations & Training Latent Spaces in Deep Convolutional Autoencoding Neural Networks \n",
    "<font color=\"red\"> This exercise work **must** be your own work. A declaration of originality is required. </font>\n",
    "\n",
    "**There will be a compulsory lab question on this exercise in the May 2020 degree exam: the exam paper will comprise a total of _4 questions_ of which _3 questions_ are to be answered by the candidate as follows:**\n",
    "\n",
    "* **One question is compulsory** and examines this Assessed Exercise.\n",
    "* **Two questions** are to be selected by the candidate from the remaining three questions set in the paper.\n",
    "\n",
    "**Make sure you complete this exercise, and understand the material thoroughly.**\n",
    "\n",
    "Do **_not_** copy and paste code from elsewhere into the exercise, **except** for code from the CVMA (H) lecture or lab notes.\n",
    "\n",
    "**READ THESE INSTRUCTIONS CAREFULLY BEFORE YOU START WORK**\n",
    "\n",
    "-----\n",
    "\n",
    "## Submission, deadline, feedback\n",
    "**Submission** Your submission will be the notebook **exercise.ipynb** with all of the code and your written work included. You should not use external files. Make sure you have run all the cells before you submit.\n",
    "\n",
    "**Deadline** You must submit this on Moodle by **5.30pm, Friday 6th December 2019**\n",
    "\n",
    "**Feeback** You will receive feedback on your submission in the New Year. There is *no mark* assigned, but you *will* get an annotated version of your notebook with individual formative feedback on your work.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Objectives\n",
    "\n",
    "1. Understand Deep Learning concepts by hands-on coding and experience.\n",
    "1. Learn PyTorch and how to construct, train and evaluate a DCNN to undertake an image transformation and an image classification task.\n",
    "1. Obtain a qualitative feel for how DCNNs operate by exploring their parameter space - “What do I need to adjust to achieve X?”\n",
    "1. Understand concepts such as overfitting/underfitting, how to diagnose this and how to address it.\n",
    "1. Understand how to combine DCNN classification and image transformation methods within a combined network formulation.\n",
    "1. Understand and gain experience in instrumenting and diagnosing networks using Tensorbaord.\n",
    "\n",
    "\n",
    "## Introduction: Autoencoders & Classifiers\n",
    "Lecture 11 describes the operation of feedforward networks such as an **autoencoder**, a network that takes in an image, forms a latent space by applying a number of encoding layers, usually convolutions, until a **bottleneck** layer is reached and then the process reverses using a **generator** that expands each layer until the original image size is reached. This network is trained by measuring the loss between the output image and the input image and optimising the net weights uintil this loss is minimised. However, by adapting either the loss function and/or the training data this architecture can be utilised to **transform** images or process them. For example we shall explore using an autoencoder to remove noise from an image by trainign the network to generate noise-free images from noise corrupted images by comparing ground truth images to noise corrupted images in the loss function.\n",
    "\n",
    "Lecture 11 also describes the standard feed forward **classifier** network which derives a latent space bottleneck in the same way as the autoencoder above. We shall explore combining the autoencoder network with the classifier network by inputing the autoencoder bottleneck latent space layer to both a decoder network and also the fully connected classificationlayers of the classifier network. The underlying hypothesis is that by training a network to accomplish more than one task, a \"better\", i.e. more generic and robust latent space can be learned to improve the performance of both the autoencoder/transformer and classifier networks. \n",
    "\n",
    "A further objective of this exercise is to understand how the classification/image generation accuracy and loss produced when training an DCNN vary according to the key parameters of the network, such as it's layer structure, activation function, level of inter-layer dropout and pooling method. The number of training cycles and batch size is also explored in this context. Use of the Tensorboard package is strongly reccommended to facilitate this investigation.\n",
    "\n",
    "Accordingly, the focus of this exercise is exploring  DCNN properties, as opposed to programming, hence the coding required has been kept to a minimum. However, a degree of literacy with the relevant Python packages (Pytorch, matplotlib etc.) is expected.\n",
    "\n",
    "-----\n",
    "### Specification\n",
    "\n",
    "This exercise is based on reference to the materials in Lab 5, the tutorials and notebooks on the PyTorch website on Deep Learning and Pytorch, and the codes in the lectures and the assessed exercise notebook iteself. \n",
    "\n",
    "**Phase 1:**\n",
    "1. Run the the basic DCNN to classify the FashionMNIST data set supplied in the exercise notebook, more details at: https://github.com/zalandoresearch/fashion-mnist\n",
    "1. Run the the basic DCNN to autoencode the FashionMNIST data set, supplied in file.\n",
    "1. Modify the above codes to plot out each network's loss during training as a function of training epoch and in the case of the classifier, also plot the classification accuracy. Note: The plots of loss (and accuracy) should be overlaid in the same display frame, with different colours for the loss and accuracty respectively so that their relationship can be compared directly as a function of training epoch.\n",
    "\n",
    "1. Investigate and compare the **baseline** performance of this system, i.e. in the above unmodified system, to when it has been modified by:\n",
    " 1. Adding/removing layers: CNN layers and fully connected layers\n",
    " 1. Varying batch sizes\n",
    " 1. Varying numbers of training epochs\n",
    " 1. Applying different levels of drop-out\n",
    " 1. Applying different types of pooling\n",
    " 1. Applying different types of activation function\n",
    "1. Tensorboard is a package which provdes a direct method for investigating network paramaters. An examples of using Tensoboard with Colab can be found in the supplied notebook: Tensorboard tryout_v2.ipynb\n",
    "1. If you aren't using Tensorboard, automate your investigation by writing a jupyter-notebook script to _automate_ the process. \n",
    " 1. Document your investigation as you undertake it. \n",
    " 1. Due to the limited time available for this lab, a reltively coarse, sequential (rather than _combinatorial_) exploration of the parameters is acceptable. \n",
    " 1. Plot your baseline and best accuracy and loss results.\n",
    "\n",
    "**Phase 2**\n",
    "1. Now let's explore combining the classification network and autoencoder networks into a single system:\n",
    " 1. Start witht the basic autoencoder from Phase 1 and connect the flattened bottleneck layer of this to the fully connected layers of the classifier network (adjusting the layer sizes as required). The input convolutional layers of the classifier are of course no longer required.\n",
    " 1. The code that computes the classifier loss is still required of course in order to devise a loss function that adds the classifier loss and the autoencoder loss together to form a _joint loss function_. This should include a parameter that weights how much of each loss (classsification versus encoder) contributes to the _total loss_ used to train the network. \n",
    " 1. Include a software switch in your combined loss function such that the network can be trained with the classifier loss only, the encoder loss only or the combined loss.\n",
    " 1. Verify that your network performs as before with individual losses and manages to learn using the combined loss.\n",
    " 1. You may require to scale the individual loss functions to be compatible since the range of the classifier losses may be very different from the range of the encoder losses. Therefore the larger loss range will \"swamp\" the samller loss range and prevent learning form happening effictively for both networks.\n",
    " 1. A simple way of doing sclaing losses is to take the loss values for a trainign run in single loss mode (either classification or autoencoding) and compute the mean and standard deviation for these loss values. Now scale the individual losses (prior to combining them) by subtracting the mean loss and then dividing by the standard deviation computed for the prior run. \n",
    " 1. Of course, prior to computing the combined loss, scale the classifier loss using the classifier mean and standard deviation and similarly scale the autoencoder loss using the autoencoder mean and standard deviation loss values.\n",
    "1. Undertake an exploration of the new combined network's parameter space and compare with the best parameterised standard system, plotting your baseline and best accuracy and loss results as in Phase 1 above. \n",
    "1. Compare the performance of the individual and combined networks using your \"loss switch\".\n",
    "\n",
    "**Phase 3**\n",
    "1. Let's now attempt to employ the new combined network to carry out an image processing task, in this case image denoising.\n",
    " 1. When training the network now add a quantity of noise to the traning image, say 1% by magnitude.\n",
    " 1. When computing the autoencoder's loss function, compare the autoencoder's output with the corresponding input image which has **NOT** been corrupted with image noise. Therefore, the network should learn how to suppress noise in the reconstructed image when a noise corrupted image is input.\n",
    " 1. Investigate optimising this system for a range of levels of noise input and network parameters.\n",
    " 1. Plot your baseline (intial unoptimised network) and best accuracy and loss results.\n",
    " 1. Report on the  investigation and interpret the results.\n",
    "\n",
    "**Phase 4**\n",
    "1. Consider your own imnage transformation (could be applying a threshold or segmentation algorithm, or a colour space transformation, or transforming monochrome images into estimated colour images as output) and train and optimise the combined network to carry out this task.\n",
    " 1. Report on the  investigation and interpret the results.\n",
    "\n",
    "\n",
    "### Evaluation\n",
    "The codes supplied in the exercise notebook and in the PyTorch tutorials and documentation.\n",
    " \n",
    "*Use Tensorbaord and/or Python scripts to automate the experimentation process.*\n",
    "\n",
    "Discuss your results in prose. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "------\n",
    "## Report\n",
    "You will need to document your approach clearly in prose, with appropriate use of figures and diagrams to show **_and interpret_**  what you have done. See below for the report outline and details on how to fill it out. Explain your steps clearly and precisely, but do not write pages of text. **Keep it short and to the point.**  Make *sure* you have run all cells before submission -- I need to be able to see the output in the notebook.\n",
    "\n",
    "Be brief, but explain yourself clearly. A paragraph or two of text for each section is sufficient, but make good use of images and graphs, especially in the results section. \n",
    "\n",
    "**You should combine your actual code with the text of the report in a literate programming style -- e.g. the approach section should contain the processing code, with interspersed Markdown cells documenting what you did. In the results sections, use Markdown cells to explain and interpret what you are showing, and use code cells to actually plot the result.**\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "#### Data Sets\n",
    "As an alternative to employing the Fashion-MNIST data set you may also select your own preferred data set, however be aware that this may require more time to train networks accordingly. Only do this if you feel confident about the task.\n",
    "\n",
    "\n",
    "### Coding hints\n",
    "* Use `torch`, `skimage`, `scipy.ndimage` or `OpenCV` to perform image manipulations. \n",
    "* Dont forget to copy the python utility funcions supplied with the lectures as required.\n",
    "* PyTorch turorials and torch.nn documentation essential as well.\n",
    "* **If you have problems understanding something - ask the lecturer or the lab assistant!**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
